{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to train a single model(mainly for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T06:50:23.433640Z",
     "iopub.status.busy": "2025-10-03T06:50:23.432893Z",
     "iopub.status.idle": "2025-10-03T06:50:30.181297Z",
     "shell.execute_reply": "2025-10-03T06:50:30.180335Z",
     "shell.execute_reply.started": "2025-10-03T06:50:23.433611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single-Model Training Script: Train on Kaggle, Upload to Hugging Face Hub\n",
    "# This script is designed to be run in a Kaggle Notebook. It trains a model\n",
    "# for a single, specified city and uploads the artifacts to a Hugging Face repo.\n",
    "#This is mainly for testing purpose\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\"Could not retrieve HF_TOKEN from Kaggle secrets.\") from e\n",
    "\n",
    "# --- Define the specific city and repositories ---\n",
    "CITY_FILE_PATH = \"w_d_1/Abbigeri.csv\" \n",
    "\n",
    "CITY_NAME = os.path.basename(CITY_FILE_PATH).replace('.csv', '').split('_')[0].lower()\n",
    "\n",
    "KAGGLE_DATASET_ID = \"mukeshdevrath007/indian-5000-cities-weather-data\"\n",
    "HF_MODEL_REPO_ID = \"Shreyansh1718/Weather-prediction-model\"\n",
    "\n",
    "KAGGLE_INPUT_PATH = f\"/kaggle/input/{KAGGLE_DATASET_ID.split('/')[1]}\"\n",
    "TEMP_ARTIFACTS_DIR = f\"/kaggle/working/{CITY_NAME}_artifacts\"\n",
    "\n",
    "# --- Setup Hugging Face API Login ---\n",
    "logger.info(\"Configuring Hugging Face credentials...\")\n",
    "api = HfApi()\n",
    "HfFolder.save_token(HF_TOKEN)\n",
    "api.create_repo(repo_id=HF_MODEL_REPO_ID, repo_type=\"model\", exist_ok=True)\n",
    "logger.info(f\"Credentials configured. Model repository '{HF_MODEL_REPO_ID}' is ready.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def create_features(df):\n",
    "    \"\"\"Create time-based and ML features for model training.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.set_index('date')\n",
    "    df = df.rename(columns={'tavg': 'temperature_2m', 'prcp': 'precipitation'})\n",
    "    df = df.ffill()\n",
    "    df['month'] = df.index.month\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['year'] = df.index.year\n",
    "    df['temp_lag_1'] = df['temperature_2m'].shift(1)\n",
    "    df['temp_lag_2'] = df['temperature_2m'].shift(2)\n",
    "    if 'precipitation' in df.columns:\n",
    "        df['precip_lag_1'] = df['precipitation'].shift(1)\n",
    "    df['temp_rolling_mean_7'] = df['temperature_2m'].shift(1).rolling(window=7).mean()\n",
    "    return df.dropna()\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \n",
    "    full_data_path = os.path.join(KAGGLE_INPUT_PATH, CITY_FILE_PATH)\n",
    "    \n",
    "    if not os.path.exists(full_data_path):\n",
    "        raise FileNotFoundError(f\"The specified city file was not found at: {full_data_path}\")\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Reading data from '{full_data_path}'...\")\n",
    "        df = pd.read_csv(full_data_path)\n",
    "        \n",
    "        logger.info(\"Data loaded successfully. Starting feature engineering...\")\n",
    "        df_processed = create_features(df)\n",
    "        \n",
    "        if len(df_processed) < 100:\n",
    "            raise ValueError(\"Insufficient data after processing.\")\n",
    "\n",
    "        y = df_processed['temperature_2m']\n",
    "        X = df_processed.drop('temperature_2m', axis=1)\n",
    "        X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "        \n",
    "        logger.info(f\"Training model for {CITY_NAME} on {len(X_train)} samples...\")\n",
    "        model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42, verbosity=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, model.predict(X_test))\n",
    "        logger.info(f\"Model trained. Validation MAE: {mae:.2f}°C\")\n",
    "        \n",
    "        os.makedirs(TEMP_ARTIFACTS_DIR, exist_ok=True)\n",
    "        model.save_model(os.path.join(TEMP_ARTIFACTS_DIR, \"model.json\"))\n",
    "        with open(os.path.join(TEMP_ARTIFACTS_DIR, \"metadata.json\"), 'w') as f:\n",
    "            json.dump({'validation_mae': mae, 'city_name': CITY_NAME, 'features': list(X_numeric.columns)}, f)\n",
    "        \n",
    "        logger.info(f\"Uploading artifacts to '{HF_MODEL_REPO_ID}' under path '{CITY_NAME}'...\")\n",
    "        api.upload_folder(\n",
    "            folder_path=TEMP_ARTIFACTS_DIR,\n",
    "            path_in_repo=CITY_NAME,\n",
    "            repo_id=HF_MODEL_REPO_ID,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"\\n✅ Successfully trained and uploaded model for {CITY_NAME}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Pipeline failed for {CITY_NAME}. Error: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if os.path.exists(TEMP_ARTIFACTS_DIR):\n",
    "            shutil.rmtree(TEMP_ARTIFACTS_DIR)\n",
    "        logger.info(\"Cleanup complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to upload trained models(large size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T09:31:23.901040Z",
     "iopub.status.busy": "2025-10-04T09:31:23.900645Z",
     "iopub.status.idle": "2025-10-04T09:31:32.284543Z",
     "shell.execute_reply": "2025-10-04T09:31:32.280253Z",
     "shell.execute_reply.started": "2025-10-04T09:31:23.901015Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "api.upload_large_folder(\n",
    "    folder_path=\"/kaggle/working/upload_staging\",\n",
    "    repo_id=\"Shreyansh1718/Weather-prediction-model\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to check number of models in model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T17:14:37.907456Z",
     "iopub.status.busy": "2025-10-04T17:14:37.907037Z",
     "iopub.status.idle": "2025-10-04T17:14:38.694471Z",
     "shell.execute_reply": "2025-10-04T17:14:38.693248Z",
     "shell.execute_reply.started": "2025-10-04T17:14:37.907419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "import logging\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# --- Configuration ---\n",
    "# Setup logging to show the script's progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# The Hugging Face Model Hub repository you want to check\n",
    "HF_MODEL_REPO_ID = \"Shreyansh1718/Weather-prediction-model\"\n",
    "\n",
    "# Get your Hugging Face token from an environment variable. A 'read' token is sufficient.\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\"Could not retrieve HF_TOKEN from Kaggle secrets. Please ensure it is set.\") from e\n",
    "\n",
    "# --- Main Verification Logic ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Connects to a Hugging Face Model Hub repository and lists all the\n",
    "    subdirectories (trained models) it contains.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Connecting to repository: {HF_MODEL_REPO_ID}...\")\n",
    "\n",
    "    try:\n",
    "        # Instantiate the HfApi client with your token\n",
    "        api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "        # Get a list of all files and folders in the repository\n",
    "        repo_contents = api.list_repo_tree(repo_id=HF_MODEL_REPO_ID, repo_type=\"model\")\n",
    "        \n",
    "        # Identify directories by checking for items that do NOT have a 'size' attribute\n",
    "        model_folders = sorted([item.path for item in repo_contents if not hasattr(item, 'size')])\n",
    "\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(f\"✅ Verification Complete\")\n",
    "        logger.info(f\"Found {len(model_folders)} model folders in the repository.\")\n",
    "        logger.info(\"=\"*50)\n",
    "\n",
    "        if model_folders:\n",
    "            logger.info(\"Sample of models found (first 20):\")\n",
    "            for i, folder_name in enumerate(model_folders[:20]):\n",
    "                logger.info(f\"  {i+1}. {folder_name}\")\n",
    "        else:\n",
    "            logger.warning(\"No model folders were found in the repository.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ An error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to generate training report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T17:12:50.810137Z",
     "iopub.status.busy": "2025-10-04T17:12:50.809764Z",
     "iopub.status.idle": "2025-10-04T17:12:51.845678Z",
     "shell.execute_reply": "2025-10-04T17:12:51.844583Z",
     "shell.execute_reply.started": "2025-10-04T17:12:50.810109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 4408 cities in dataset\n",
      "Found 4408 cities in dataset\n",
      "Found 4596 top-level items in HF repo\n",
      "Found 4596 top-level items in HF repo\n",
      "Wrote training report to /kaggle/working/upload_staging/training_report.json\n",
      "Wrote training report to /kaggle/working/upload_staging/training_report.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verify trained models against dataset CSVs.\n",
    "\n",
    "This script is designed to run after the training pipeline on Kaggle. It:\n",
    "- Scans the Kaggle input dataset for city CSV files\n",
    "- Lists top-level uploaded model folders in the specified HF model repository\n",
    "- Produces a `training_report.json` in the upload staging directory with lists:\n",
    "  - `dataset_cities`: all city names found in data files\n",
    "  - `uploaded_models`: model names found in HF repo\n",
    "  - `missing_models`: cities present in dataset but not in repo (need training)\n",
    "  - `extra_models`: models in repo that don't match dataset cities\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Set\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "logger = logging.getLogger(\"verify_trained_models\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "KAGGLE_DATASET_ID = os.getenv(\"KAGGLE_DATASET_ID\", \"mukeshdevrath007/indian-5000-cities-weather-data\")\n",
    "KAGGLE_INPUT_PATH = os.getenv(\"KAGGLE_INPUT_PATH\", f\"/kaggle/input/{KAGGLE_DATASET_ID.split('/')[1]}\")\n",
    "UPLOAD_STAGING_DIR = os.getenv(\"UPLOAD_STAGING_DIR\", \"/kaggle/working/upload_staging\")\n",
    "HF_MODEL_REPO_ID = os.getenv(\"HF_MODEL_REPO_ID\", \"Shreyansh1718/Weather-prediction-model\")\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "def discover_dataset_city_names(input_root: str, subfolders: List[str]) -> Set[str]:\n",
    "    cities = set()\n",
    "    for sub in subfolders:\n",
    "        full = os.path.join(input_root, sub)\n",
    "        if not os.path.isdir(full):\n",
    "            continue\n",
    "        for fname in os.listdir(full):\n",
    "            if fname.lower().endswith('.csv'):\n",
    "                name = fname.replace('.csv', '')\n",
    "                city = name.split('_')[0].split('-')[0].lower()\n",
    "                cities.add(city)\n",
    "    return cities\n",
    "\n",
    "\n",
    "def list_repo_top_level(api: HfApi, repo_id: str) -> Set[str]:\n",
    "    try:\n",
    "        tree = api.list_repo_tree(repo_id=repo_id, repo_type='model')\n",
    "        top = set(item.path.split('/')[0].lower() for item in tree)\n",
    "        return top\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not list repo tree: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not HF_TOKEN:\n",
    "        logger.warning('HF_TOKEN not provided; API calls may be rate-limited or unauthenticated')\n",
    "\n",
    "    api = HfApi(token=HF_TOKEN) if HF_TOKEN else HfApi()\n",
    "\n",
    "    candidate_subfolders = ['.', 'w_d_1', 'w_d_2', 'w_d_3/w_d_3', 'dn']\n",
    "    dataset_cities = discover_dataset_city_names(KAGGLE_INPUT_PATH, candidate_subfolders)\n",
    "    logger.info(f\"Found {len(dataset_cities)} cities in dataset\")\n",
    "\n",
    "    uploaded_models = list_repo_top_level(api, HF_MODEL_REPO_ID)\n",
    "    logger.info(f\"Found {len(uploaded_models)} top-level items in HF repo\")\n",
    "\n",
    "    missing = sorted(list(dataset_cities - uploaded_models))\n",
    "    extra = sorted(list(uploaded_models - dataset_cities))\n",
    "\n",
    "    os.makedirs(UPLOAD_STAGING_DIR, exist_ok=True)\n",
    "    report_path = os.path.join(UPLOAD_STAGING_DIR, 'training_report.json')\n",
    "    report = {\n",
    "        'dataset_cities': sorted(list(dataset_cities)),\n",
    "        'uploaded_models': sorted(list(uploaded_models)),\n",
    "        'missing_models': missing,\n",
    "        'extra_models': extra,\n",
    "    }\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Wrote training report to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to check uploaded models and train remaining models and upload them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T19:47:01.053352Z",
     "iopub.status.busy": "2025-10-04T19:47:01.052280Z",
     "iopub.status.idle": "2025-10-04T19:47:14.920114Z",
     "shell.execute_reply": "2025-10-04T19:47:14.919452Z",
     "shell.execute_reply.started": "2025-10-04T19:47:01.053278Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Automated Multi-City Training Pipeline\n",
    "\n",
    "This script discovers city CSVs in the Kaggle dataset input path, trains a separate XGBoost model per city,\n",
    "and uploads trained artifacts to a Hugging Face model repo. It fixes the bug where previously-detected\n",
    "models in the repo were not correctly compared against city names, which caused retraining or skipping errors.\n",
    "\n",
    "Designed to run in a Kaggle environment but also runnable locally if HF_TOKEN is provided as an env var.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from huggingface_hub import HfApi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# --- TQDM-friendly logging handler ---\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "logger = logging.getLogger(\"train_and_upload_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.addHandler(TqdmLoggingHandler())\n",
    "\n",
    "# --- Config ---\n",
    "TARGET_COLUMN = \"temperature_2m\"\n",
    "MAX_CITIES_TO_PROCESS = 5000\n",
    "\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "KAGGLE_DATASET_ID = os.getenv(\"KAGGLE_DATASET_ID\", \"mukeshdevrath007/indian-5000-cities-weather-data\")\n",
    "KAGGLE_INPUT_PATH = os.getenv(\"KAGGLE_INPUT_PATH\", f\"/kaggle/input/{KAGGLE_DATASET_ID.split('/')[1]}\")\n",
    "UPLOAD_STAGING_DIR = os.getenv(\"UPLOAD_STAGING_DIR\", \"/kaggle/working/upload_staging\")\n",
    "\n",
    "HF_MODEL_REPO_ID = os.getenv(\"HF_MODEL_REPO_ID\", \"Shreyansh1718/Weather-prediction-model\")\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.rename(columns={\"tavg\": \"temperature_2m\", \"prcp\": \"precipitation\"})\n",
    "    df = df.ffill()\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"day_of_year\"] = df.index.dayofyear\n",
    "    df[\"day_of_week\"] = df.index.dayofweek\n",
    "    df[\"year\"] = df.index.year\n",
    "    df[\"temp_lag_1\"] = df[\"temperature_2m\"].shift(1)\n",
    "    df[\"temp_lag_2\"] = df[\"temperature_2m\"].shift(2)\n",
    "    if \"precipitation\" in df.columns:\n",
    "        df[\"precip_lag_1\"] = df[\"precipitation\"].shift(1)\n",
    "    df[\"temp_rolling_mean_7\"] = df[\"temperature_2m\"].shift(1).rolling(window=7).mean()\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def list_csv_files_in_input(root: str, subfolders: List[str]) -> List[str]:\n",
    "    \"\"\"Search given subfolders (relative to root) and return relative CSV file paths found.\"\"\"\n",
    "    results = []\n",
    "    for sub in subfolders:\n",
    "        full = os.path.join(root, sub)\n",
    "        if not os.path.isdir(full):\n",
    "            logger.warning(f\"Data subfolder not found: {full}\")\n",
    "            continue\n",
    "        for fname in os.listdir(full):\n",
    "            if fname.lower().endswith(\".csv\"):\n",
    "                results.append(os.path.join(sub, fname))\n",
    "    return sorted(results)\n",
    "\n",
    "\n",
    "def normalize_city_name_from_path(path: str) -> str:\n",
    "    \"\"\"Derive a canonical city id/name from a filename like 'Jaipur_daily_...csv' -> 'jaipur'.\"\"\"\n",
    "    base = os.path.basename(path)\n",
    "    name = base.replace(\".csv\", \"\")\n",
    "    city = name.split(\"_\")[0].split(\"-\")[0].lower()\n",
    "    return city\n",
    "\n",
    "\n",
    "def get_existing_repo_models(api: HfApi, repo_id: str) -> Set[str]:\n",
    "    \"\"\"Return a set of model names already present in the HF model repo.\n",
    "\n",
    "    We expect models uploaded under top-level folders with the city name. If repo is empty\n",
    "    or the HF API call fails, return an empty set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = api.list_repo_tree(repo_id=repo_id, repo_type=\"model\")\n",
    "        top_level = set()\n",
    "        for item in tree:\n",
    "            parts = item.path.split(\"/\")\n",
    "            top_level.add(parts[0].lower())\n",
    "        return top_level\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not list repo tree: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def train_and_save_city_model(city_name: str, city_df: pd.DataFrame) -> bool:\n",
    "    try:\n",
    "        df_processed = create_features(city_df)\n",
    "        if len(df_processed) < 100:\n",
    "            logger.warning(f\"Insufficient data for {city_name}: {len(df_processed)} rows\")\n",
    "            return False\n",
    "\n",
    "        y = df_processed[TARGET_COLUMN]\n",
    "        X = df_processed.drop(TARGET_COLUMN, axis=1)\n",
    "        X_numeric = X.select_dtypes(include=[\"number\"]).fillna(0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "        model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42, verbosity=0)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, model.predict(X_test))\n",
    "        logger.info(f\"Model trained for {city_name}. Validation MAE: {mae:.2f}°C\")\n",
    "\n",
    "        city_artifact_path = os.path.join(UPLOAD_STAGING_DIR, city_name)\n",
    "        os.makedirs(city_artifact_path, exist_ok=True)\n",
    "\n",
    "        model.save_model(os.path.join(city_artifact_path, \"model.json\"))\n",
    "        with open(os.path.join(city_artifact_path, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump({\"validation_mae\": mae, \"city_name\": city_name, \"features\": list(X_numeric.columns)}, f)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train {city_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    # locate HF token if running on Kaggle\n",
    "    if not HF_TOKEN:\n",
    "        try:\n",
    "            # Kaggle secrets are only available in Kaggle kernels; attempt to import locally if present\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not HF_TOKEN:\n",
    "        logger.warning(\"HF_TOKEN not found in env or Kaggle secrets. The script will still train locally but cannot upload.\")\n",
    "\n",
    "    api = HfApi(token=HF_TOKEN) if HF_TOKEN else HfApi()\n",
    "\n",
    "    # prepare staging dir\n",
    "    if os.path.exists(UPLOAD_STAGING_DIR):\n",
    "        shutil.rmtree(UPLOAD_STAGING_DIR)\n",
    "    os.makedirs(UPLOAD_STAGING_DIR, exist_ok=True)\n",
    "\n",
    "    # determine existing models in repo\n",
    "    existing_models = get_existing_repo_models(api, HF_MODEL_REPO_ID)\n",
    "    logger.info(f\"Found {len(existing_models)} top-level entries in repo (assumed existing city models)\")\n",
    "\n",
    "    # Load a training report that lists missing models to train.\n",
    "    training_report_path = '/kaggle/working/upload_staging/training_report.json'\n",
    "    city_items = []  \n",
    "\n",
    "    if os.path.exists(training_report_path):\n",
    "        try:\n",
    "            logger.info(f\"Loading training report from {training_report_path}\")\n",
    "            with open(training_report_path, 'r') as f:\n",
    "                report = json.load(f)\n",
    "            missing = set([c.lower() for c in report.get('missing_models', [])])\n",
    "\n",
    "            # discover CSVs and filter by missing list\n",
    "            candidate_subfolders = ['.', 'w_d_1', 'w_d_2', 'w_d_3/w_d_3', 'dn']\n",
    "            csv_paths = list_csv_files_in_input(KAGGLE_INPUT_PATH, candidate_subfolders)\n",
    "            logger.info(f\"Discovered {len(csv_paths)} CSV files in input.\")\n",
    "\n",
    "            for rel in csv_paths:\n",
    "                city = normalize_city_name_from_path(rel)\n",
    "                if city in missing:\n",
    "                    city_items.append((city, rel))\n",
    "\n",
    "            if not city_items:\n",
    "                logger.info(\"Training report present but no matching CSVs found for missing models. Falling back to discovery of all new cities.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load or parse training report ({training_report_path}): {e}. Falling back to discovery.\")\n",
    "\n",
    "    # Fallback\n",
    "    if not city_items:\n",
    "        candidate_subfolders = ['.', 'w_d_1', 'w_d_2', 'w_d_3/w_d_3', 'dn']\n",
    "        csv_paths = list_csv_files_in_input(KAGGLE_INPUT_PATH, candidate_subfolders)\n",
    "        all_city_filepaths = csv_paths\n",
    "        logger.info(f\"Discovered {len(all_city_filepaths)} CSV files in input.\")\n",
    "\n",
    "        for rel in all_city_filepaths:\n",
    "            city = normalize_city_name_from_path(rel)\n",
    "            if city in existing_models:\n",
    "                logger.info(f\"Skipping {city} - already present in repo\")\n",
    "                continue\n",
    "            city_items.append((city, rel))\n",
    "\n",
    "    files_to_run = city_items[:MAX_CITIES_TO_PROCESS]\n",
    "    logger.info(f\"{len(files_to_run)} cities scheduled for training in this run.\")\n",
    "\n",
    "    successful = 0\n",
    "    for city_name, rel_path in tqdm(files_to_run, desc=\"Training\"):\n",
    "        full_path = os.path.join(KAGGLE_INPUT_PATH, rel_path)\n",
    "        try:\n",
    "            df = pd.read_csv(full_path)\n",
    "            if train_and_save_city_model(city_name, df):\n",
    "                successful += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {city_name} from {full_path}: {e}\")\n",
    "\n",
    "    if successful > 0 and HF_TOKEN:\n",
    "        logger.info(f\"Uploading {successful} trained city artifacts to {HF_MODEL_REPO_ID}...\")\n",
    "        try:\n",
    "            api.upload_large_folder(folder_path=UPLOAD_STAGING_DIR, repo_id=HF_MODEL_REPO_ID, repo_type=\"model\")\n",
    "            logger.info(\"Upload complete.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Upload failed: {e}\")\n",
    "    elif successful > 0:\n",
    "        logger.warning(\"Trained models present locally in staging directory but HF_TOKEN missing, skipping upload.\")\n",
    "\n",
    "    logger.info(f\"Done. {successful}/{len(files_to_run)} trained successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4805309,
     "sourceId": 8158130,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
